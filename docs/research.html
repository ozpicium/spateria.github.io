
<!DOCTYPE HTML>
<html lang="en">
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
    
    </script>

    <script async defer src="https://buttons.github.io/buttons.js">
    </script>

	
  <title>Shubham Pateria</title>
  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <meta name="author" content="Shubham Pateria">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">

	
</head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
    	      
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:0%;width:70%;vertical-align:middle">
                <div style="display: flex; justify-content: space-between; align-items: center;">
            		  <p style="font-size: 250%; font-family: verdana; margin-top: 20px;">
            		    <name> Shubham Pateria </name>
            		  </p>
            		  <div style="margin-top: 20px;">
			    <a href="."><b>Home</b></a> &nbsp;|&nbsp;
            		    <a class="cool-link" href="research"><b>Research</b></a> &nbsp;|&nbsp;
			    <a class="cool-link" href="patent_others"><b>Patents and Projects</b></a> &nbsp;|&nbsp;
			    <a class="cool-link" href="services"><b>Services</b></a>
            		  </div>
            		</div>
            		<hr>

              </td>
            </tr>
          </tbody></table>
	
	
     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			
		  <p style="font-size:150%;font-family:verdana"> <name> <b>Research Projects and Publications</b> </name> </p>

	     <tr onmouseout="thesis_stop()" onmouseover="thesis_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/thesis.jpg' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://dr.ntu.edu.sg/handle/10356/155182"> <b>Methods for Autonomously Decomposing and Performing Long-horizon Sequential Decision Tasks</b> </a>
			</p>
				<u><b>Shubham Pateria</b></u> <br>
			<p>
				[<b>DR-NTU</b>] <i>Ph.D. Thesis in Digital Repository of NTU, 2022.</i>
			<p>

				[<a href="data/explainART_paper.pdf"> slides</a>]
				[<a href="data/explainART_paper.pdf"> video</a>]
			</p>
			 </td>			
		</tr>
	     
	      <tr onmouseout="hisoma_stop()" onmouseover="hisoma_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/hisoma.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://sites.google.com/smu.edu.sg/cognitiveandneuralcomputing/projects#h.vj0pjwaa76rz"> <b>Hierarchical Multi-agent Control using Deep Reinforcement Learning and Self-organizing Neural Networks</b> </a>
			</p>
				Minghong Geng, <u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan. <br>
			<p>
				[<b>Confidential</b>] <i>Licensed to DSO National Laboratories, Singapore in 2023.</i>
			<p>
				A hierarchical multi-agent reinforcement learning system combining self-organizing and deep neural networks, for simulated defense research technology licensed to DSO National Laboratories. The codes and documentation related to this research are currently confidential.
			</p>
			 </td>			
		</tr>
	
		<tr onmouseout="explainART_stop()" onmouseover="explainART_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/explainART.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://dl.acm.org/doi/abs/10.5555/3545946.3598922"> <b>Towards Explaining Sequences of Actions in Multi-Agent Deep Reinforcement Learning Models</b> </a>
			</p>
				Khaing Phyo Wai, Minghong Geng, Budhitama Subagdja, <u><b>Shubham Pateria</b></u>, Ah-Hwee Tan. <br>
			<p> 
				[<b>AAMAS 2023</b>] <i>In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS '23), IFAAMAS, Richland, SC, 2325–2327.</i> <br>
			<p> 	
				[<a href="data/explainART_paper.pdf"> poster</a>]  
			</p> 
			<p>
				A novel approach to explain Multi-agent Deep Reinforcement Learning (MADRL) by transforming sequences of action events performed by agents into high-level abstract strategies using a spatio-temporal neural network model.
			</p>
			 </td>			
		</tr>
	
		<tr onmouseout="lsgvp_stop()" onmouseover="lsgvp_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/lsgvp.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://ieeexplore.ieee.org/abstract/document/10040536"> <b>LSGVP: Value-Based Subgoal Discovery and Path Planning for Reaching Long-Horizon Goals</b> </a>
			</p>
				<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek. <br>
			<p> 
				[<b>IEEE TNNLS 2023</b>] <i>In IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3240004; (Impact Factor: 14.255).</i> <br>
			<p> 	
				[<a href="data/lsgvp_paper.pdf"> paper</a>]  
			</p> 
			<p>
				LSGVP is a subgoal-graph planning method for goal-based navigation and simulated robot control. It uses a subgoal discovery heuristic that is based on a cumulative reward (value) measure and yields sparse subgoals, including those lying on the higher cumulative reward paths. Moreover, LSGVP guides the agent to automatically prune the learned subgoal graph to remove the erroneous edges. The combination of these novel features helps the LSGVP agent to achieve higher cumulative positive rewards than other subgoal sampling or discovery heuristics, as well as higher goal-reaching success rates than other state-of-the-art subgoal graph-based planning methods.
			</p>
			 </td>			
		</tr>
	
		<tr onmouseout="lidoss_stop()" onmouseover="lidoss_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/lidoss.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://ieeexplore.ieee.org/abstract/document/9462536"> <b>LIDOSS: End-to-End Hierarchical Reinforcement Learning With Integrated Subgoal Discovery</b> </a>
			</p>
				<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek. <br>
			<p> 
				[<b>IEEE TNNLS 2022</b>] <i>In IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 12, pp. 7778-7790, Dec. 2022, doi: 10.1109/TNNLS.2021.3087733; (Impact Factor: 14.255).</i> <br>
			<p> 	
				[<a href="data/lidoss_paper.pdf"> paper</a>]  
			</p> 
			<p>
				LIDOSS is an end-to-end Hierarchical Reinforcement Learning (HRL) method for goal-based navigation and simulated robot control. It introduces a subgoal discovery heuristic that narrows down the search space for the higher-level policy by focusing on subgoals that are more likely to occur in state-transitions leading to the goal. Evaluated against a state-of-the-art HRL method called Hierarchical Actor Critic (HAC), LIDOSS demonstrates improved goal achievement rates across various continuous control tasks.
			</p>
			 </td>			
		</tr>
	
		<tr onmouseout="hrlsurvey_stop()" onmouseover="hrlsurvey_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/hrlsurvey.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://dl.acm.org/doi/abs/10.1145/3453160"> <b>Hierarchical Reinforcement Learning: A Comprehensive Survey</b> </a>
			</p>
				<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek. <br>
			<p> 
				[<b>ACM CSUR 2022</b>] <i>In ACM Computing Surveys 54, 5, Article 109 (June 2022), 35 pages. https://doi.org/10.1145/3453160; (Impact Factor: 14.324).</i> <br>
			<p> 	
				[<a href="data/hrlsurvey_paper.pdf"> paper</a>]  
			</p> 
			<p>
				We provide a survey of both foundational as well as recent Hierarchical Reinforcement Learning (HRL) approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL.
			</p>
			 </td>			
		</tr>
	
		<tr onmouseout="hrlsurvey_stop()" onmouseover="hrlsurvey_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/lidossold.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://dl.acm.org/doi/abs/10.5555/3398761.3399042"> <b>Hierarchical Reinforcement Learning with Integrated Discovery of Salient Subgoals</b> </a>
			</p>
				<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan. <br>
			<p> 
				[<b>AAMAS 2020</b>] <i>In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS '20). IFAAMAS, Richland, SC, 1963–1965.</i> <br>
			<p> 	
				[<a href="data/lidossold_paper.pdf"> paper</a>]  
			</p> 
			 </td>			
		</tr>
		      
		<tr onmouseout="isemo_stop()" onmouseover="isemo_start()">
		    <td style="padding:10px;width:25%;vertical-align:middle">
		      <div class="one">
			<img src='images/isemo.png' width="210">
		      </div>
		    </td>
	            <td style="padding:10px;width:75%;vertical-align:middle">
			<p> <a href="https://ieeexplore.ieee.org/abstract/document/9002777"> <b>ISEMO: Multi-agent Reinforcement Learning in Spatial Domain Tasks using Inter Subtask Empowerment Rewards</b> </a>
			</p>
				<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan. <br>
			<p> 
				[<b>IEEE SSCI 2019</b>] <i>In IEEE Symposium Series on Computational Intelligence (SSCI), Xiamen, China, 2019, pp. 86-93, doi: 10.1109/SSCI44817.2019.9002777.</i> <br>
			<p> 	
				[<a href="data/isemo_paper.pdf"> paper</a>] 
				[<a href="https://github.com/spateria/ISEMO/tree/master"> codes</a>]
			</p> 
			<p>
				ISEMO is a Multi-agent Hierarchical Reinforcement Learning (MAHRL) that uses an Inter-Subtask Empowerment Reward (ISER) to facilitate cooperation among agents in complex tasks, where some agents are essential for reaching rewarding states while others play supporting roles. ISER complements task rewards, enhancing inter-agent coordination, and ISEMO includes an options model to learn subtask termination functions, improving flexibility compared to hand-crafted conditions. Experiments demonstrate that ISEMO effectively learns subtask policies and terminations, outperforming standard MAHRL in a spatial Search and Rescue scenario.
			</p>
			 </td>			
		</tr>
		      
	    </tbody></table>
	  <p>
	  <p>
  </body>
</html>
