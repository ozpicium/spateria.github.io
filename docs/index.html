
<!DOCTYPE HTML>
<html lang="en">
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
    
    </script>

    <script async defer src="https://buttons.github.io/buttons.js">
    </script>

	
  <title>Shubham Pateria</title>
  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <meta name="author" content="Shubham Pateria">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">

	
</head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
    	      
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <td style="padding:0%;width:70%;vertical-align:middle">
                <p style="font-size:300%;text-align:center"> <name> Shubham Pateria </name> </p>
                <p> 
                  I am a Research Scientist at <a href="https://www.smu.edu.sg/">Singapore Management University (SMU), Singapore</a> in the <a href="https://sites.google.com/smu.edu.sg/cognitiveandneuralcomputing/home?authuser=0">Neural and Cognitive Computing Group</a>, since April, 2022. My work is primarily focused on multi-agent reinforcement learning and <a href="https://en.wikipedia.org/wiki/Federated_learning">federated learning</a>. 
                </p>
                <p>
                  At SMU, I am a key contributor to the <a href="https://sites.google.com/smu.edu.sg/cognitiveandneuralcomputing/projects#h.vj0pjwaa76rz">Scalable and Interpretable iCGF</a> project funded by <a href="https://www.dso.org.sg/">DSO National Laboaratories</a> and <a href="https://sites.google.com/smu.edu.sg/cognitiveandneuralcomputing/projects#h.1jb4lx4ixurb">Trustworthy Federated Ubiquitous Learning (TrustFUL)</a> project funded by <a href="https://aisingapore.org/">AISG</a>. Prior to this, I was a part of <a href="">Entrepreneur First</a>, Singapore startup incubator from which I briefly worked on my own mental health technology startup before shutting it down in early 2022.
                </p>
                <p>
                  I received my Doctor of Philosophy (Ph.D.) degree in Computer Science from <a href="https://www.ntu.edu.sg/">Nanyang Technological University (NTU)</a>, Singapore in 2022, where I was adviced by <a href="https://sites.google.com/smu.edu.sg/ahtan/home">Prof. Ah-Hwee Tan</a> and <a href="https://scholar.google.com.sg/citations?user=Tj-KsfQAAAAJ&hl=en">Assoc. Prof. Chai Quek</a>. 
                </p>
                <p>
                  Prior to Ph.D., I was working as a Senior Software Engineer at <a href="https://research.samsung.com/sri-b">Samsung Research India - Bangalore</a> on display driver software and innovative research on smartphone and smarthome system optimization. 
                </p>
                <p>
                  I received my Bachelor of Technology (B.Tech.) degree in Electronics and Communications Engineering from <a href="https://nitdgp.ac.in/">National Institute of Technology (NIT), Durgapur</a>, India in 2013.
                </p>
                <p style="text-align:center">
                  <a href="mailto:shubhamp@smu.edu.sg">Email</a> &nbsp;/&nbsp;
                  <a href="data/ShubhamPateria_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=PuJ5C_QAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/shubham-pateria-2091/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/spateria/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/shubhamp.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/shubhamp.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
	
          
	  
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
	<p style="font-size:200%"> <name> Research </name> </p>
        <p>
           I'm interested in single-agent and multi-agent reinforcement learning, machine learning in general, and federated learning for preserving data privacy.
        </p>
     
    
      <tr onmouseout="hisoma_stop()" onmouseover="hisoma_start()">
	    <td style="padding:10px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/hisoma.png' width="210">
	      </div>
	    </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
		<p> <a href="https://sites.google.com/smu.edu.sg/cognitiveandneuralcomputing/projects#h.vj0pjwaa76rz"> Hierarchical Multi-agent Control using Deep Reinforcement Learning and Self-organizing Neural Networks </a>
		</p>
			Minghong Geng, <u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan. <br>
		<p>
			[<b>Confidential</b>] Licensed to DSO National Laboratories, Singapore in 2023.
		<p>
			A hierarchical multi-agent reinforcement learning system combining self-organizing and deep neural networks, for simulated defense research technology licensed to DSO National Laboratories. The codes and documentation related to this research is currently confidential.
		</p>
		 </td>			
	</tr>

	<tr onmouseout="explainART_stop()" onmouseover="explainART_start()">
	    <td style="padding:10px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/explainART.png' width="210">
	      </div>
	    </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
		<p> <a href="https://dl.acm.org/doi/abs/10.5555/3545946.3598922"> Towards Explaining Sequences of Actions in Multi-Agent Deep Reinforcement Learning Models [Poster] </a>
		</p>
			Khaing Phyo Wai, Minghong Geng, Budhitama Subagdja, <u><b>Shubham Pateria</b></u>, Ah-Hwee Tan. <br>
		<p> 
			[<b>AAMAS 2023</b>] In the 2023 International Conference on Autonomous Agents and Multiagent Systems. <br>
		<p> 	
			[<a href="data/explainART_paper.pdf"> poster</a>]  
		</p> 
		<p>
			A novel approach to explain Multi-agent Deep Reinforcement Learning (MADRL) by transforming sequences of action events performed by agents into high-level abstract strategies using a spatio-temporal neural network model.
		</p>
		 </td>			
	</tr>

	<tr onmouseout="lsgvp_stop()" onmouseover="lsgvp_start()">
	    <td style="padding:10px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/lsgvp.png' width="210">
	      </div>
	    </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
		<p> <a href="https://ieeexplore.ieee.org/abstract/document/10040536"> LSGVP: Value-Based Subgoal Discovery and Path Planning for Reaching Long-Horizon Goals </a>
		</p>
			<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek. <br>
		<p> 
			[<b>IEEE TNNLS 2023</b>] In IEEE Transactions on Neural Networks and Learning Systems, doi: 10.1109/TNNLS.2023.3240004; (Impact Factor: 14.255). <br>
		<p> 	
			[<a href="data/lsgvp_paper.pdf"> paper</a>]  
		</p> 
		<p>
			LSGVP is a subgoal-graph planning method for goal-based navigation and simulated robot control. It uses a subgoal discovery heuristic that is based on a cumulative reward (value) measure and yields sparse subgoals, including those lying on the higher cumulative reward paths. Moreover, LSGVP guides the agent to automatically prune the learned subgoal graph to remove the erroneous edges. The combination of these novel features helps the LSGVP agent to achieve higher cumulative positive rewards than other subgoal sampling or discovery heuristics, as well as higher goal-reaching success rates than other state-of-the-art subgoal graph-based planning methods.
		</p>
		 </td>			
	</tr>

	<tr onmouseout="lidoss_stop()" onmouseover="lidoss_start()">
	    <td style="padding:10px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/lidoss.png' width="210">
	      </div>
	    </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
		<p> <a href="https://ieeexplore.ieee.org/abstract/document/9462536"> LIDOSS: End-to-End Hierarchical Reinforcement Learning With Integrated Subgoal Discovery </a>
		</p>
			<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek. <br>
		<p> 
			[<b>IEEE TNNLS 2022</b>] In IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 12, pp. 7778-7790, Dec. 2022, doi: 10.1109/TNNLS.2021.3087733; (Impact Factor: 14.255). <br>
		<p> 	
			[<a href="data/lidoss_paper.pdf"> paper</a>]  
		</p> 
		<p>
			LIDOSS is an end-to-end Hierarchical Reinforcement Learning (HRL) method for goal-based navigation and simulated robot control. It introduces a subgoal discovery heuristic that narrows down the search space for the higher-level policy by focusing on subgoals that are more likely to occur in state-transitions leading to the goal. Evaluated against a state-of-the-art HRL method called Hierarchical Actor Critic (HAC), LIDOSS demonstrates improved goal achievement rates across various continuous control tasks.
		</p>
		 </td>			
	</tr>

	<tr onmouseout="hrlsurvey_stop()" onmouseover="hrlsurvey_start()">
	    <td style="padding:10px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/hrlsurvey.png' width="210">
	      </div>
	    </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
		<p> <a href="https://dl.acm.org/doi/abs/10.1145/3453160"> Hierarchical Reinforcement Learning: A Comprehensive Survey </a>
		</p>
			<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek. <br>
		<p> 
			[<b>ACM CSUR 2022</b>] In ACM Computing Surveys 54, 5, Article 109 (June 2022), 35 pages. https://doi.org/10.1145/3453160; (Impact Factor: 14.324). <br>
		<p> 	
			[<a href="data/hrlsurvey_paper.pdf"> paper</a>]  
		</p> 
		<p>
			We provide a survey of both foundational as well as recent Hierarchical Reinforcement Learning (HRL) approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL.
		</p>
		 </td>			
	</tr>
	      
	<tr onmouseout="isemo_stop()" onmouseover="isemo_start()">
	    <td style="padding:10px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/isemo.png' width="210">
	      </div>
	    </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
		<p> <a href="https://ieeexplore.ieee.org/abstract/document/9002777"> ISEMO: Multi-agent Reinforcement Learning in Spatial Domain Tasks using Inter Subtask Empowerment Rewards </a>
		</p>
			<u><b>Shubham Pateria</b></u>, Budhitama Subagdja, Ah-Hwee Tan. <br>
		<p> 
			[<b>IEEE SSCI 2019</b>] In IEEE Symposium Series on Computational Intelligence (SSCI), Xiamen, China, 2019, pp. 86-93, doi: 10.1109/SSCI44817.2019.9002777. <br>
		<p> 	
			[<a href="data/isemo_paper.pdf"> paper</a>] 
			[<a href="https://github.com/spateria/ISEMO/tree/master"> codes</a>]
		</p> 
		<p>
			ISEMO is a Multi-agent Hierarchical Reinforcement Learning (MAHRL) that uses an Inter-Subtask Empowerment Reward (ISER) to facilitate cooperation among agents in complex tasks, where some agents are essential for reaching rewarding states while others play supporting roles. ISER complements task rewards, enhancing inter-agent coordination, and ISEMO includes an options model to learn subtask termination functions, improving flexibility compared to hand-crafted conditions. Experiments demonstrate that ISEMO effectively learns subtask policies and terminations, outperforming standard MAHRL in a spatial Search and Rescue scenario.
		</p>
		 </td>			
	</tr>
	      
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
               <p><heading>Other Works</heading> </p>
		<p> 
	     <ul class="b">
		<li> <a href="https://dl.acm.org/doi/10.1145/3554980">ClusterFL: A Clustering-based Federated Learning System for Human Activity Recognition </a> [<a href="https://dl.acm.org/doi/pdf/10.1145/3554980"> paper</a>] </li>
			 <u><b>Xiaomin Ouyang</b></u>, Zhiyuan Xie, Jiayu Zhou, Guoliang Xing, Jianwei Huang<br>
		<i>  [<b>TOSN</b>] ACM Transactions on Sensor Networks  </i><br> </p>

		<li> <a href="https://dl.acm.org/doi/abs/10.1145/3485730.3485946"> FedDL: Federated Learning via Dynamic Layer Sharing for Human Activity Recognition </a> 
		[<a href="https://dl.acm.org/doi/abs/10.1145/3485730.3485946"> paper</a>] 
		[<a href="https://www.youtube.com/watch?v=zLDGgouZAYA"> video</a>]
		[<a href="https://github.com/xmouyang/xmouyang.github.io/blob/master/data/FedDL_SenSys_slides.pptx"> sldies</a>]
		</li>
			 Linlin Tu, <u><b>Xiaomin Ouyang</b></u>, Jiayu Zhou, Yuze He, Guoliang Xing <br>
		<i>  [<b>SenSys 2021</b>] In the 19th ACM Conference on Embedded Networked Sensor Systems (Acceptance ratio: 25/139=17.9%) </i><br> </p>

		<li> <a href="https://dl.acm.org/doi/abs/10.1145/3485730.3485927"> UltraDepth: Exposing High-resolution Texture from Depth Cameras </a> 
		[<a href="https://dl.acm.org/doi/abs/10.1145/3485730.3485927"> paper</a>] 
		[<a href="https://www.youtube.com/watch?v=hlqN5W8CpwM"> video</a>]
		[<a href="https://github.com/xmouyang/xmouyang.github.io/blob/master/data/UltraDepth-SenSys-slides.pdf"> sldies</a>]
		</li>
			 Zhiyuan Xie, <u><b>Xiaomin Ouyang</b></u>, Xiaoming Liu, Guoliang Xing <br>
		<i>  [<b>SenSys 2021</b>] In the 19th ACM Conference on Embedded Networked Sensor Systems (Acceptance ratio: 25/139=17.9%) </i><br> </p>
		     
		<li> <a href="https://dl.acm.org/doi/10.1145/3495243.3558747"> HiToF: A ToF Camera System for Capturing High-Resolution Textures </a> 
			[<a href="https://dl.acm.org/doi/pdf/10.1145/3495243.3558747"> paper</a>]
			</li>
			Zhiyuan Xie, <u><b>Xiaomin Ouyang</b></u>, Li Pan, Wenrui Lu, Xiaoming Liu, Guoliang Xing <br>
		<i> [<b>MobiCom 2022 Demo</b>] In the 28th Annual International Conference On Mobile Computing And Networking</i><br></p>
	
		<li>Optimization Design for Federated Learning in Heterogeneous 6G Networks
			[<a href="https://arxiv.org/pdf/2303.08322.pdf"> paper</a>]
	</li>
		Bing Luo, <u><b>Xiaomin Ouyang</b></u>, Peng Sun, Pengchao Han, Ningning Ding, Jianwei Huang<br>
		<i>Accepted by IEEE Network, 2023.</i><br></p>
		 </td>
        </tbody></table>
	  
  </body>
</html>
